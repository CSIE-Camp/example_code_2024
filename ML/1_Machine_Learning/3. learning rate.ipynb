{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **讀入資料**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 可以從第一個實作複製過來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO：貼上網址抓資料\n",
    "url = \".../chinese_science_score.csv\"\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **計算 gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 設定 x, y（上一個實作一樣有）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['chinese']\n",
    "y = data['science']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 計算 gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 100\n",
    "b = 100\n",
    "\n",
    "a_gradient = 2 * x * (a * x + b - y)\n",
    "b_gradient = 2 * (a * x + b - y)\n",
    "\n",
    "# TODO：計算平均結果\n",
    "a_gradient = (2 * x * (a * x + b - y)).sum()\n",
    "b_gradient = (2 * (a * x + b - y)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 包成 function 方便計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, a, b):\n",
    "\n",
    "    a_gradient = (2 * x * (a * x + b - y)).mean()\n",
    "    b_gradient = (2 * (a * x + b - y)).mean()\n",
    "\n",
    "    return a_gradient, b_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 測試 compute_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gradient(x, y, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 觀察 a, b 的變動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 100\n",
    "b = 100\n",
    "a_gradient, b_gradient = compute_gradient(x, y, a, b)\n",
    "\n",
    "learning_rate = 0.0000001\n",
    "\n",
    "# TODO：移動 a, b 的數值（learning rate）\n",
    "a = ()\n",
    "b = ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **觀察是否 loss 值有下降**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> loss function（上一個實作有程式碼）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, y, a, b):\n",
    "    y_pred = a*x + b\n",
    "    loss = (y - y_pred)**2\n",
    "    loss = loss.sum() / len(x)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 觀察 loss 是否下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "\n",
    "# TODO：印出 loss 值\n",
    "\n",
    "\n",
    "a_gradient, b_gradient = compute_gradient(x, y, a, b)\n",
    "\n",
    "learning_rate = 0.0000001\n",
    "a = a - a_gradient * learning_rate\n",
    "b = b - b_gradient * learning_rate\n",
    "\n",
    "# TODO：印出 loss 值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上面只有 1 次，現在試試看 10 次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 100\n",
    "b = 100\n",
    "learning_rate = 0.0000001\n",
    "\n",
    "for i in range(10):\n",
    "    a_gradient, b_gradient = compute_gradient(x, y, a, b)\n",
    "\n",
    "    a = a - a_gradient * learning_rate\n",
    "    b = b - b_gradient * learning_rate\n",
    "\n",
    "    loss = compute_loss(x, y, a, b)\n",
    "    \n",
    "    print(f\"第 {i:3} 次更新: Loss: {loss:.2f}, a: {a:.2f}, b: {b:.2f}, a_gradient: {a_gradient:.2f}, b_gradient: {b_gradient:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 包成一個 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, a, b, learning_rate, loss_function, gradient_function, run_iter, show_iter):\n",
    "    # 把過程中的都記錄下來\n",
    "    loss_hist = []\n",
    "    a_hist = []\n",
    "    b_hist = []\n",
    "    \n",
    "    for i in range(run_iter):\n",
    "        \n",
    "        a_gradient, b_gradient = gradient_function(x, y, a, b)\n",
    "\n",
    "        a = a - a_gradient * learning_rate\n",
    "        b = b - b_gradient * learning_rate\n",
    "\n",
    "        loss = loss_function(x, y, a, b)\n",
    "\n",
    "        loss_hist.append(loss)\n",
    "        a_hist.append(a)\n",
    "        b_hist.append(b)\n",
    "\n",
    "        if i%show_iter == 0:\n",
    "            print(f\"第 {i:3} 次更新: Loss: {loss:.2f}, a: {a:.2f}, b: {b:.2f}, a_gradient: {a_gradient:.2f}, b_gradient: {b_gradient:.2f}\") # 增加斜率\n",
    "    \n",
    "    # 回傳 a, b 最終值、過程中的 a, b, loss\n",
    "    return a, b, a_hist, b_hist, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 測試 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 100\n",
    "b = 100\n",
    "learning_rate = 0.0000001\n",
    "run_iter = 10000\n",
    "show_iter = 2000\n",
    "\n",
    "a_final, b_final, a_hist, b_hist, loss_hist = gradient_descent(x, y, a, b, learning_rate, compute_loss, compute_gradient, run_iter, show_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 查看最終 a 跟 b 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"最終 a, b = ({a_final}, {b_final})\")\n",
    "# 顯示兩位小數就好\n",
    "print(f\"最終 a, b = ({a_final:.2f}, {b_final:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **畫圖觀察 loss 趨勢**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TODO：畫出所有 loss_hist 值\n",
    "plt.plot(np.arange(0, run_iter), )\n",
    "\n",
    "plt.title(\"次數 vs loss\")\n",
    "plt.xlabel(\"次數\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 中文字體（同樣從前面複製）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import fontManager\n",
    "import matplotlib as mlp\n",
    "fontManager.addfont(\"ChineseFont.ttf\")\n",
    "mlp.rc('font', family = \"ChineseFont\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **畫圖觀察 3D 的移動軌跡**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 窮舉所有 loss 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a_all = np.arange(-100, 101)\n",
    "b_all = np.arange(-100, 101)\n",
    "loss_all = np.zeros((201, 201))\n",
    "\n",
    "i = 0\n",
    "for a in a_all:\n",
    "    j = 0\n",
    "    for b in b_all:\n",
    "        loss = compute_loss(x, y, a, b)\n",
    "        loss_all[i, j] = loss\n",
    "        j = j+1\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 把畫 3D 圖 的程式碼複製過來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6)) # 圖片大小\n",
    "\n",
    "ax = plt.axes(projection = \"3d\")\n",
    "ax.view_init(20, -100) # 查看視角\n",
    "ax.xaxis.set_pane_color((1, 1, 1)) # 背景顏色\n",
    "ax.yaxis.set_pane_color((1, 1, 1))\n",
    "ax.zaxis.set_pane_color((1, 1, 1))\n",
    "\n",
    "b_grid, a_grid = np.meshgrid(b_all, a_all) # 網格化\n",
    "######   https://wangyeming.github.io/2018/11/12/numpy-meshgrid/\n",
    "ax.plot_surface(a_grid, b_grid, loss_all, alpha=0.3)\n",
    "\n",
    "# 設定資訊\n",
    "ax.set_title(\"a b 對應的 loss\")\n",
    "ax.set_xlabel(\"a\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_zlabel(\"loss\")\n",
    "\n",
    "# 尋找 loss 最少的點\n",
    "print(np.min(loss_all))\n",
    "a_index, b_index = np.where(loss_all == np.min(loss_all))\n",
    "print(a_index, b_index, a_all[a_index], b_all[b_index])\n",
    "ax.scatter(a_all[a_index], b_all[b_index], loss_all[a_index, b_index], color=\"red\", s=40)\n",
    "\n",
    "# TODO：劃出軌跡\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 把 gradient_descent() 移下來方便操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 100\n",
    "b = 100\n",
    "learning_rate = 0.0000001\n",
    "run_iter = 10000\n",
    "show_iter = 2000\n",
    "\n",
    "a_final, b_final, a_hist, b_hist, loss_hist = gradient_descent(x, y, a, b, learning_rate, compute_loss, compute_gradient, run_iter, show_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
